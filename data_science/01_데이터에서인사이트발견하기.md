# 01_데이터에서 인사이트 발견하기

> 우리는 왜 데이터를 분석하려고 할까? 바로 문제해결을 위한 Insight 를 얻기 위해서

## 1.1 탐색적 데이터 분석의 과정

> 데이터 분석의 기본인 EDA (Exploratory Data Analysis)

### 1. 탐색 ( 데이터의 기초 정보 살펴보기 )

1. 데이터의 출처와 주제에 대해 이해

2. 데이터의 크기 확인

3. 데이터의 구성 요소 (피처) 살펴보기

   - 피처: 데이터를 구성하는 요소

     ![스크린샷 2020-12-12 오전 4.15.39](../images/picher.png)



### 2. 인사이트 발견

- 피처의 속성 탐색

- 피처 간의 상관 관계 탐색



### 3. 탐색한 데이터의 시각화



## 1.2 멕시코풍 프랜차이즈 chipotle 의 주문 데이터 분석하기

> 주문데이터를 불러와 기초정보 살펴보고 탐색 / 시각화를 수행해보고 이 예제로 전처리를 해보고 이런저런 인사이트를 살펴보자
>
> 흐름은 순차적으로 내가 원하고자 하는 넓은 범위 조건의 df 를 만들고, 계속해서 df 를 만들어가면서 결국 원하는 specific 한 조건의 df 를 만드는 듯

### Step 1. 탐색: 데이터의 기초 정보 살펴보기

- 불러오기

  - Csv / tsp 등 파일을 df 객체로 바꿔주자
  
- 정보확인

  - 전체적인 정보, 통계적 정보 등등을 파악할 수 있다.
  
- 수치적 특성들 파악

  - 수치의 요약적인 통계량 파악가능
  - 피처의 value 수를 중복제거하여 파악할 수 있다.
  
  

### Step2. 인사이트의 발견: 탐색과 시각화

- 가장 많이 주문한 item top 10
- Value_count() 사용
- Item 당 주문 개수
- groupby() 사용
- 아이템당 주문개수 시각화

  - matplot 라이브러리 사용

- ! Value_counts() 와 unique() 의 차이
  - Value_count() 는 series 를 반환
  - Unique() 는 numpy 배열을 반환



### Step3. 데이터 전처리

- Item_price 피처가 $--- 인 value 이기 때문에, apply 함수를 이용해 $를 없앤 int64 로 바꿔줄 수 있다.




### Step4. 이런저런 인사이트

- 주문당 평균 계산금액

  - groupby('order_id') 중 [item_price] 의 sum()의 mean()
  
- 한 주문에 10달러 이상 사용한 주문 번호 출력

  - Groupby('order_id') 의 sum() 중 [그 중 item_price 가 10 이상인 것]
  
- 각 아이템의 가격 구하기

  - Groupby('item_name') 의 min() 중 ['item_price']
  
- 아이템 가격 분포 그래프 출력

  - plt 를 이용해서 x_pos 를 각 아이템으로 주고 y_pos 를 item_prce 를 holist() 해줘서 넣어주자
  
- 가장 비싼 주문에서 아이템의 개수가 몇개인지

  - Groupby('order_id') 의 sum() 을 sort_values(by='item_price', ascending=False) 해주자

- "Veggie Salad Bowl" 이 몇 번 주문되었는지 확인

  - 조건문에서 'item_name' 이 'veggie salad bowl' 인 것만 따로 df 를 만들어주고 drop_duplicates('item_name', 'order_id') 해서 중복제거
  
- Chicken Bowl 을 2개 이상 주문한 주문 횟수 수하기. 

  - Quantity 가 2 이상인 것
  
- Chicken Bowl 을 2 개 이상 주문한 고객들의 "Chicken Bowl" 메뉴의 총 주문 수량

  - 총합을 나타내는 df 를 만든 후 그 중 값이 2 이상인 것만 조건문
  
  

## 1.3 국가별 음주 데이터 분석하기

> 위 chipo 와 비슷하니, 이제 좀 더 라이브러리에 대해 익숙해져보자

### Step 1 탐색: 데이터의 기초 정보 살펴보기

- 데이터셋 기본 정보 가져오기

- 정보들을 한 번 보자

  

### Step 2 인사이트 발견: 탐색과 시각화하기

- 피처 간 상관관계

  > #### 상관분석
  > 상관 분석이란 두 변수 간의 선형적 관계를 상관 계수로 표현하는 것
  > 상관 계수를 구하는 것은 공분산의 개념을 포함
  > 공분산: 2개의 확률 변수에 대한 상관 정도, 2개의 변수 중 하나의 값이 상승할 때 다른 값이 얼마나 상승하는지의 수치
  > But, 공분산만으로 구하면 두 변수의 단위 크기에 영향을 받을 수 있어서 -1 과 1 사이인 상관계수로 바꾼다.
  > 1에 가깝다면 양의 상관관계, -1에 가깝다면 음의 상관관계

  - df.corr(method = 'pearson') 을 주로 쓴다.

- corr 행렬 히트맵 시각화하기

  > [Seaboard.heatmap 의 parameters](https://seaborn.pydata.org/generated/seaborn.heatmap.html)

- 피처 간 scatter plot 출력

  > [seaborn.pairplot](https://seaborn.pydata.org/generated/seaborn.pairplot.html)



### Step3. 탐색적 분석: 스무고개로 분석

- continent 피처에 대한 결측 (누락) 데이터 전처리

  - na 되어 있는 value 들을 처리해줄 수 있다. Fillna('원하는값')
  
- 파이차트로 시각화

  > [plt.pie](https://matplotlib.org/3.3.4/api/_as_gen/matplotlib.pyplot.pie.html)
  >
  > **labels**
  >
  > list, default: None
  > A sequence of strings providing the labels for each wedge
  >
  > **x**
  >
  > 수치로 나타낼 값

- 대륙별 분석
  - Agg() 로 원하는 method 를 한데 모을 수 있다.
  - 전체 평균값을 따로 구해서 전체 평균보다 높은 것을 조건문 할 수 있다.
  - Idxmax() 를 이용해서 value 가 가장 높은 index 를 구할 수 있다.
- 분석결과 시각화
  - plt.xticks 으로 tick 을 몇 개나, 또 어떤 label 로 표현할지 조절할 수 있다.
  - plt.legend() : 범례
  - plt.plot() 을 이용해 그래프 위에 표시할 수 있다.
  - plt.set_color(): bar 의 색 설정 가능



### Step4. 통계적 분석: 분석 대상간의 통계적 차이 검정하기

> 지금까지 두 피처 간의 상관성이나, 그룹 단위로 나누어 수치 정보를 살펴봤다. 물론 유용하지만,
>
> 이는 분석가의 주관에 따라 도출된 내용이여서 **분석의 타당성** 에서 한계가 있다.
>
> 따라서 통계적으로 차이를 검정하는 과정이 필요하고, 그 중 가장 기본적인 방법인 t-test 를 해보자.

[통계학노트](/통계학노트.md)



#### 대한민국은 얼마나 술을 독하게 마시는 나라일까?

- 술 소비량 대비 알코올 비율 피처를 생성하면 알 수 있다.



#### 표로 정리

| 데이터 탐색 질문                               | 핵심 내용                    | 인사이트                                                     |
| ---------------------------------------------- | ---------------------------- | ------------------------------------------------------------ |
| 각각의 피처는 서로 어떤 상관 관계를 갖는가?    | 모든 연속형 피처의 상관 분석 | 대부분의 국가의 총 알코올 소비량은 맥주 소비량에 영향을 받을 확률이 높다. 또한 대부분의 국가에서는 맥주가 가장 많이 소비되는 술이라는 해석도 가능 |
| 평균 맥주 소비량이 가장 높은 대륙?             | 모든 행을 그룹 단위로 분석   | 유럽이 가장 맥주 소비량이 높음. 대륙별로 상이한 차이가 있다는 것을 발견함 |
| 술 소비량 대비 알코올 비율 피처 생성하기       | 새로운 분석 피처 생성        | 술 소비량 대비 알코올 비율이라는 새로운 피처로부터 술을 독하게 마시는 정도의 국가별 차이를 관찰 가능 |
| 아프리카와 유럽 간의 맥주 소비량 차이 **검정** | 통계적 차이 검정             | T-test 분석 결과, 아프리카와 유럽 간의 맥주 소비량은 통계적으로 유의미한 차이를 보임 (단, 그룹 간 데이터 크기가 매우 다르고, 정규분포를 띤다는 가정을 할 수 없어 신뢰할 만한 정보라고 할 수 없음.) |



# 02 텍스트 마이닝 첫걸음

> 비정형 데이터 중 텍스트 데이터로부터 유의미한 정보를 추출하는 텍스트 마이닝
> 	비정형: 정해진 형태가 없고, 연산이 불가능한 상태



## 핵심개념

1. 웹 크롤링으로 데이터를 수집
2. 키워드 추출의 방법
3. 키워드 간의 연관 관계 분석
4. 텍스트 분석 결과 시각화



## 2.1 웹크롤링으로 기초 데이터 수집하기

### 대상 페이지의 구조 살펴보기

> 가장 먼저 리스트의 url 정보를 수집해야 하는데 이 과정을 python 으로 자동화 하여 웹 크롤러를 만들자

### 웹 크롤링 라이브러리 사용하기

> 원하는 태그의 원하는 속성을 자동으로 가져오자

- request 라이브러리
  - 특정 url 로부터 html 문서를 가져옴
- BeautifulSoup4 모듈
  - Html 문서에서 데이터를 추출

```shell
pip3 install lxml beautifulsoup4 requests
```

### 텍스트 정보 수집하기

> get() 함수 말고 text() 함수를 사용해 태그의 텍스트 정보를 추출



## 2.2 나무위키 최근 변경 페이지 키워드 분석하기

### step 1. 크롤링: 웹데이터 가져오기

### step 2. 추출: 키워드 정보 추출하기

1. 수집한 데이터에서 키워드 정보만 추출하기 위해 **텍스트 전처리** 수행
   - 파이썬에서 're' 라는 모듈로 정규표현식으로 한글 이외의 문자는 전부 제거가능
   - 이는 텍스트 마이닝의 목적에 따라 다를 수 있음
2. 명사 혹은 형태소 단위의 문자열로 **키워드추출** 을 위해 **말뭉치** 를 만들어 리스트 반환
3. 해당 말뭉치들을 konlpy 라이브러리의 Okt 를 이용해 Counter 객체에 {'단어':'빈도'...} 형태로 만들 수 있음
   - 여기서 의미적인 독립이 불가능한 품사인 불용어를 불용어 사전과 비교해 없애줌

## step 3. 시각화: 워드 클라우드 시각화

> 워드 클라우드 라이브러리 많은데 그 중 pytagcloud 를 많이 씀

## 정리

| 핵심내용                           | 설명                                                         |
| ---------------------------------- | ------------------------------------------------------------ |
| 정규표현식을 활용한 텍스트 전처리  | 텍스트 분석 방향에 맞는 분자를 선별. 예제에서는 한글을 추출하기 위해 정규표현식 사용 |
| 형태소 분석기를 활용한 키워드 추출 | Konlpy 등의 형태소 분석기를 통해 데이터에서 키워드 추출      |
| 불용어 사전 적용                   | 많이 등장하지만 실질적 의미가 없는 불용어를 데이터에서 제거  |



## 2.3 특정 키워드가 있는 게시물 크롤링을 위해 트위터 API 사용

### 트위터 API 등록

1. 트위터 개발자 계정 등록
2. 개발자 앱 등록
   1. Key & tokens 얻어냄

### 파이썬 API 설정

> tweepy 라이브러리

발급된 트위터 key 들을 넣어주어 사용

## 2.4 트위터 API 로 '손흥민' 과 관련된 키워드 분석

- **연관분석**: 데이터의 집합으로부터 특정한 규칙 찾아냄, 아래 연관분석을 위한 평가지표들

  > 조건절을 A, 결과절을 B 라고 하자

  - 지지도(Support)

    - 전체중 A 와 B 가 동시에 일어날 확률
    - 둘다 포함하는 거래 수 / 전체 거래 수

  - 신뢰도(Confidence)

    - A 가 일어났다는 가정하에 B 가 일어날 확률
    - 지지도(Support) / P(A)

  - 향상도(Lift):

    - 임의로 B 가 발생하는 경우에 비해 A 와의 관계가 고려되어 구매되는 경우의 비율. 즉, A 가 B 를 예측하기 위해 능력이 얼마나 향상되었는가

    - 신뢰도(Confidence) / P(B)

      - | Lift   | 의미                | 예             |
        | ------ | ------------------- | -------------- |
        | 1      | 두 사건이 서로 독립 | 과자, 후추     |
        | 1 초과 | 양의 상관 관계      | 빵, 버터       |
        | 1 미만 | 음의 상관 관계      | 설사약, 변비약 |



### step 1. API 호출: 트위터 API 로 데이터 가져오기

> 데이터를 가져와 DF 형태로 정리하자

### step 2. 추출: 키워드 추출하기

1. 데이터 전처리
2. 피처간 연관분석에 용이하기 위해 **하나의 열 데이터 단위**로 키워드 추출

### step 3. 분석: 연관 분석을 이용한 키워드 분석

> apyori 라이브러리로 생성 가능한 모든 연관 규칙 중 빈발집합(Frequent sets) 만을 우선적으로 고려
> 	초월집합(Superset) 개념을 도입해 규칙의 형태를 제한

1. 연관분석 평가지표들로 조건을 설정해 연관분석 수행
2. 조건절과 결과절, 지지도 를 갖는 DF 생성
3. 빈도수가 50 미만인 키워드 제거하여 노드명과 빈도수를 갖는 DF 생성

### step 4. 시각화: 연관 키워드 네트워크 시각화하기

## 정리

| 핵심내용              | 설명                                                         |
| --------------------- | ------------------------------------------------------------ |
| 연관 규칙 분석        | 트랜잭션 데이터에 연관 규칙을 적용하고, 키워드 간의 지지도, 신뢰도, 향상도를 검토 |
| Apriori 알고리즘 적용 | 큰 규모의 데이터를 처리하기 위해서는 Apriori 와 같은 알고리즘을 도입. superset  을 이용해 빈도가 낮은 하위 집합을 가지치기 |

